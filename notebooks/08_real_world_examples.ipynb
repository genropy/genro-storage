{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Use Cases\n",
    "\n",
    "**Duration:** 40 minutes  \n",
    "**Level:** Advanced\n",
    "\n",
    "Complete, practical examples you can adapt for your projects.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Automated backup system\n",
    "- Multi-environment deployment\n",
    "- Report generation pipeline\n",
    "- File upload processing\n",
    "- Log aggregation\n",
    "- Static site deployment\n",
    "- Data migration\n",
    "\n",
    "These are production-ready patterns! ðŸ’¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genro_storage import StorageManager\n",
    "import tempfile\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "storage = StorageManager()\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "storage.configure([\n",
    "    {'name': 'local', 'type': 'local', 'path': temp_dir},\n",
    "    {'name': 'mem', 'type': 'memory'}\n",
    "])\n",
    "\n",
    "print(\"âœ“ Storage ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Automated Backup System\n",
    "\n",
    "Complete backup solution with rotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackupManager:\n",
    "    \"\"\"Automated backup with rotation and verification\"\"\"\n",
    "    \n",
    "    def __init__(self, storage, source_path, backup_base, max_backups=7):\n",
    "        self.storage = storage\n",
    "        self.source = storage.node(source_path)\n",
    "        self.backup_base = storage.node(backup_base)\n",
    "        self.max_backups = max_backups\n",
    "    \n",
    "    def create_backup(self):\n",
    "        \"\"\"Create timestamped backup\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        backup_dir = self.backup_base.child(timestamp)\n",
    "        backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"Creating backup: {timestamp}\")\n",
    "        \n",
    "        # Incremental backup (skip unchanged)\n",
    "        stats = {'copied': 0, 'skipped': 0, 'bytes': 0}\n",
    "        \n",
    "        def on_copy(src, dst):\n",
    "            stats['copied'] += 1\n",
    "            stats['bytes'] += src.size\n",
    "        \n",
    "        def on_skip(src, dst):\n",
    "            stats['skipped'] += 1\n",
    "        \n",
    "        self.source.copy_to(\n",
    "            backup_dir,\n",
    "            skip='hash',\n",
    "            on_file=on_copy,\n",
    "            on_skip=on_skip\n",
    "        )\n",
    "        \n",
    "        # Create manifest\n",
    "        manifest = {\n",
    "            'timestamp': timestamp,\n",
    "            'source': self.source.fullpath,\n",
    "            'files_copied': stats['copied'],\n",
    "            'files_skipped': stats['skipped'],\n",
    "            'bytes_copied': stats['bytes']\n",
    "        }\n",
    "        \n",
    "        manifest_file = backup_dir.child('backup_manifest.json')\n",
    "        manifest_file.write_text(json.dumps(manifest, indent=2))\n",
    "        \n",
    "        print(f\"  âœ“ Copied: {stats['copied']} files ({stats['bytes']} bytes)\")\n",
    "        print(f\"  âŠ˜ Skipped: {stats['skipped']} files\")\n",
    "        \n",
    "        return backup_dir\n",
    "    \n",
    "    def rotate_backups(self):\n",
    "        \"\"\"Remove old backups\"\"\"\n",
    "        backups = sorted([c for c in self.backup_base.children() if c.isdir])\n",
    "        \n",
    "        if len(backups) > self.max_backups:\n",
    "            to_remove = backups[:-self.max_backups]\n",
    "            print(f\"\\nRemoving {len(to_remove)} old backups\")\n",
    "            for backup in to_remove:\n",
    "                print(f\"  Deleting: {backup.basename}\")\n",
    "                backup.delete()\n",
    "    \n",
    "    def list_backups(self):\n",
    "        \"\"\"List all backups with info\"\"\"\n",
    "        backups = sorted([c for c in self.backup_base.children() if c.isdir])\n",
    "        \n",
    "        print(f\"\\nAvailable backups ({len(backups)}):\")\n",
    "        for backup in backups:\n",
    "            manifest_file = backup.child('backup_manifest.json')\n",
    "            if manifest_file.exists:\n",
    "                manifest = json.loads(manifest_file.read_text())\n",
    "                print(f\"  {backup.basename}: {manifest['files_copied']} files\")\n",
    "            else:\n",
    "                print(f\"  {backup.basename}\")\n",
    "\n",
    "# Example usage\n",
    "print(\"Example: Backup system\")\n",
    "\n",
    "# Create test data\n",
    "data_dir = storage.node('local:important_data')\n",
    "data_dir.mkdir()\n",
    "for i in range(5):\n",
    "    data_dir.child(f'file_{i}.txt').write_text(f'Important data {i}')\n",
    "\n",
    "# Setup backup manager\n",
    "bm = BackupManager(\n",
    "    storage,\n",
    "    'local:important_data',\n",
    "    'local:backups',\n",
    "    max_backups=3\n",
    ")\n",
    "\n",
    "# Create first backup\n",
    "bm.create_backup()\n",
    "\n",
    "# Modify data\n",
    "data_dir.child('file_2.txt').write_text('Modified data')\n",
    "\n",
    "# Create second backup\n",
    "import time\n",
    "time.sleep(1)  # Ensure different timestamp\n",
    "bm.create_backup()\n",
    "\n",
    "# List backups\n",
    "bm.list_backups()\n",
    "\n",
    "# Rotate (if needed)\n",
    "bm.rotate_backups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Environment Deployment\n",
    "\n",
    "Deploy configs to dev/staging/prod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeploymentManager:\n",
    "    \"\"\"Deploy configurations to multiple environments\"\"\"\n",
    "    \n",
    "    def __init__(self, storage):\n",
    "        self.storage = storage\n",
    "        self.environments = {\n",
    "            'dev': 'mem:deploy/dev',\n",
    "            'staging': 'mem:deploy/staging',\n",
    "            'prod': 'mem:deploy/prod'\n",
    "        }\n",
    "    \n",
    "    def deploy_config(self, config_node, env, dry_run=False):\n",
    "        \"\"\"Deploy configuration to environment\"\"\"\n",
    "        if env not in self.environments:\n",
    "            raise ValueError(f\"Unknown environment: {env}\")\n",
    "        \n",
    "        dest_path = self.environments[env]\n",
    "        dest = self.storage.node(dest_path)\n",
    "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nDeploying to {env}:\")\n",
    "        \n",
    "        if dry_run:\n",
    "            print(f\"  [DRY RUN] Would copy {config_node.fullpath}\")\n",
    "            print(f\"            to {dest.fullpath}\")\n",
    "            return\n",
    "        \n",
    "        # Backup current if exists\n",
    "        if dest.exists:\n",
    "            backup = self.storage.node(f\"{dest_path}.backup\")\n",
    "            dest.copy_to(backup)\n",
    "            print(f\"  âœ“ Backed up current config\")\n",
    "        \n",
    "        # Deploy\n",
    "        config_node.copy_to(dest)\n",
    "        print(f\"  âœ“ Deployed: {config_node.size} bytes\")\n",
    "        \n",
    "        # Verify\n",
    "        if config_node.md5hash == dest.md5hash:\n",
    "            print(f\"  âœ“ Verified: checksums match\")\n",
    "        else:\n",
    "            raise RuntimeError(\"Deployment verification failed!\")\n",
    "    \n",
    "    def promote(self, from_env, to_env, dry_run=False):\n",
    "        \"\"\"Promote config from one env to another\"\"\"\n",
    "        source_path = self.environments[from_env]\n",
    "        source = self.storage.node(source_path)\n",
    "        \n",
    "        if not source.exists:\n",
    "            raise FileNotFoundError(f\"No config in {from_env}\")\n",
    "        \n",
    "        print(f\"\\nPromoting: {from_env} -> {to_env}\")\n",
    "        self.deploy_config(source, to_env, dry_run=dry_run)\n",
    "    \n",
    "    def rollback(self, env):\n",
    "        \"\"\"Rollback to backup\"\"\"\n",
    "        dest_path = self.environments[env]\n",
    "        dest = self.storage.node(dest_path)\n",
    "        backup = self.storage.node(f\"{dest_path}.backup\")\n",
    "        \n",
    "        if not backup.exists:\n",
    "            raise FileNotFoundError(\"No backup available\")\n",
    "        \n",
    "        print(f\"\\nRolling back {env}:\")\n",
    "        backup.copy_to(dest)\n",
    "        print(f\"  âœ“ Restored from backup\")\n",
    "\n",
    "# Example usage\n",
    "dm = DeploymentManager(storage)\n",
    "\n",
    "# Create config\n",
    "config = storage.node('mem:configs/app_config.json')\n",
    "config.parent.mkdir(parents=True, exist_ok=True)\n",
    "config.write_text(json.dumps({\n",
    "    'version': '1.0',\n",
    "    'timeout': 30\n",
    "}, indent=2))\n",
    "\n",
    "# Deploy to dev (test first)\n",
    "dm.deploy_config(config, 'dev', dry_run=True)\n",
    "dm.deploy_config(config, 'dev')\n",
    "\n",
    "# Test, then promote to staging\n",
    "dm.promote('dev', 'staging')\n",
    "\n",
    "# Test, then promote to prod\n",
    "dm.promote('staging', 'prod')\n",
    "\n",
    "print(\"\\nâœ“ Deployment pipeline complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Report Generation Pipeline\n",
    "\n",
    "Generate reports from multiple data sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportGenerator:\n",
    "    \"\"\"Generate reports from data sources\"\"\"\n",
    "    \n",
    "    def __init__(self, storage):\n",
    "        self.storage = storage\n",
    "    \n",
    "    def generate_daily_report(self, date=None):\n",
    "        \"\"\"Generate daily business report\"\"\"\n",
    "        if date is None:\n",
    "            date = datetime.now()\n",
    "        \n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        print(f\"Generating report for {date_str}...\")\n",
    "        \n",
    "        # Build report using iternode\n",
    "        builder = self.storage.iternode()\n",
    "        \n",
    "        # Header\n",
    "        header = self.storage.node('mem:_temp_header')\n",
    "        header.write_text(f\"\"\"\n",
    "=====================================\n",
    "DAILY BUSINESS REPORT\n",
    "Date: {date_str}\n",
    "=====================================\n",
    "\n",
    "\"\"\")\n",
    "        builder.append(header)\n",
    "        \n",
    "        # Sales section\n",
    "        sales_data = self._get_sales_data(date)\n",
    "        sales_section = self.storage.node('mem:_temp_sales')\n",
    "        sales_section.write_text(f\"\"\"\n",
    "SALES SUMMARY\n",
    "-------------\n",
    "Total Sales: ${sales_data['total']:,.2f}\n",
    "Orders: {sales_data['orders']}\n",
    "Average Order: ${sales_data['average']:,.2f}\n",
    "\n",
    "\"\"\")\n",
    "        builder.append(sales_section)\n",
    "        \n",
    "        # Users section\n",
    "        user_data = self._get_user_data(date)\n",
    "        user_section = self.storage.node('mem:_temp_users')\n",
    "        user_section.write_text(f\"\"\"\n",
    "USER ACTIVITY\n",
    "-------------\n",
    "New Users: {user_data['new_users']}\n",
    "Active Users: {user_data['active']}\n",
    "Total Users: {user_data['total']}\n",
    "\n",
    "\"\"\")\n",
    "        builder.append(user_section)\n",
    "        \n",
    "        # Footer\n",
    "        footer = self.storage.node('mem:_temp_footer')\n",
    "        footer.write_text(f\"\"\"\n",
    "=====================================\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "=====================================\n",
    "\"\"\")\n",
    "        builder.append(footer)\n",
    "        \n",
    "        # Save report\n",
    "        report_file = self.storage.node(f'mem:reports/daily_{date_str}.txt')\n",
    "        report_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        builder.copy_to(report_file)\n",
    "        \n",
    "        print(f\"  âœ“ Report saved: {report_file.fullpath}\")\n",
    "        print(f\"  âœ“ Size: {report_file.size} bytes\")\n",
    "        \n",
    "        return report_file\n",
    "    \n",
    "    def _get_sales_data(self, date):\n",
    "        \"\"\"Simulate getting sales data\"\"\"\n",
    "        return {\n",
    "            'total': 15234.50,\n",
    "            'orders': 87,\n",
    "            'average': 175.11\n",
    "        }\n",
    "    \n",
    "    def _get_user_data(self, date):\n",
    "        \"\"\"Simulate getting user data\"\"\"\n",
    "        return {\n",
    "            'new_users': 12,\n",
    "            'active': 543,\n",
    "            'total': 8921\n",
    "        }\n",
    "\n",
    "# Generate report\n",
    "rg = ReportGenerator(storage)\n",
    "report = rg.generate_daily_report()\n",
    "\n",
    "print(\"\\nReport content:\")\n",
    "print(report.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. File Upload Processing\n",
    "\n",
    "Process uploaded files (validation, storage, thumbnails):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UploadProcessor:\n",
    "    \"\"\"Process file uploads with validation\"\"\"\n",
    "    \n",
    "    def __init__(self, storage, upload_dir, max_size_mb=10):\n",
    "        self.storage = storage\n",
    "        self.upload_dir = upload_dir\n",
    "        self.max_size = max_size_mb * 1024 * 1024\n",
    "        \n",
    "        self.allowed_types = {\n",
    "            'image/jpeg', 'image/png', 'image/gif',\n",
    "            'application/pdf',\n",
    "            'text/plain'\n",
    "        }\n",
    "    \n",
    "    def process_upload(self, file_node, user_id):\n",
    "        \"\"\"Process an uploaded file\"\"\"\n",
    "        print(f\"\\nProcessing upload: {file_node.basename}\")\n",
    "        \n",
    "        # Validate size\n",
    "        if file_node.size > self.max_size:\n",
    "            raise ValueError(f\"File too large: {file_node.size} bytes\")\n",
    "        print(f\"  âœ“ Size ok: {file_node.size} bytes\")\n",
    "        \n",
    "        # Validate type\n",
    "        if file_node.mimetype not in self.allowed_types:\n",
    "            raise ValueError(f\"Type not allowed: {file_node.mimetype}\")\n",
    "        print(f\"  âœ“ Type ok: {file_node.mimetype}\")\n",
    "        \n",
    "        # Generate safe filename\n",
    "        import hashlib\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        hash_part = file_node.md5hash[:8]\n",
    "        safe_name = f\"{timestamp}_{hash_part}_{file_node.basename}\"\n",
    "        \n",
    "        # Store in user directory\n",
    "        dest_path = f\"{self.upload_dir}/{user_id}/{safe_name}\"\n",
    "        dest = self.storage.node(dest_path)\n",
    "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        file_node.copy_to(dest)\n",
    "        print(f\"  âœ“ Stored: {dest.fullpath}\")\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            'original_name': file_node.basename,\n",
    "            'size': file_node.size,\n",
    "            'mime_type': file_node.mimetype,\n",
    "            'md5': file_node.md5hash,\n",
    "            'uploaded_at': datetime.now().isoformat(),\n",
    "            'user_id': user_id\n",
    "        }\n",
    "        \n",
    "        meta_file = self.storage.node(f\"{dest_path}.meta.json\")\n",
    "        meta_file.write_text(json.dumps(metadata, indent=2))\n",
    "        print(f\"  âœ“ Metadata saved\")\n",
    "        \n",
    "        return dest\n",
    "    \n",
    "    def list_user_uploads(self, user_id):\n",
    "        \"\"\"List all uploads for a user\"\"\"\n",
    "        user_dir = self.storage.node(f\"{self.upload_dir}/{user_id}\")\n",
    "        \n",
    "        if not user_dir.exists:\n",
    "            return []\n",
    "        \n",
    "        uploads = []\n",
    "        for child in user_dir.children():\n",
    "            if child.suffix == '.json':\n",
    "                continue\n",
    "            \n",
    "            meta_file = self.storage.node(f\"{child.fullpath}.meta.json\")\n",
    "            if meta_file.exists:\n",
    "                metadata = json.loads(meta_file.read_text())\n",
    "                uploads.append({\n",
    "                    'file': child,\n",
    "                    'metadata': metadata\n",
    "                })\n",
    "        \n",
    "        return uploads\n",
    "\n",
    "# Example usage\n",
    "processor = UploadProcessor(storage, 'mem:uploads', max_size_mb=5)\n",
    "\n",
    "# Simulate uploaded files\n",
    "upload1 = storage.node('mem:temp/photo.jpg')\n",
    "upload1.parent.mkdir(parents=True, exist_ok=True)\n",
    "upload1.write_bytes(b'\\xFF\\xD8\\xFF' + b'fake jpeg data' * 100)\n",
    "\n",
    "upload2 = storage.node('mem:temp/document.pdf')\n",
    "upload2.write_text('PDF content here...')\n",
    "\n",
    "# Process uploads\n",
    "processor.process_upload(upload1, user_id='user123')\n",
    "processor.process_upload(upload2, user_id='user123')\n",
    "\n",
    "# List user uploads\n",
    "uploads = processor.list_user_uploads('user123')\n",
    "print(f\"\\nUser has {len(uploads)} uploads:\")\n",
    "for upload in uploads:\n",
    "    meta = upload['metadata']\n",
    "    print(f\"  - {meta['original_name']} ({meta['size']} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Log Aggregation\n",
    "\n",
    "Collect and merge logs from multiple sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogAggregator:\n",
    "    \"\"\"Aggregate logs from multiple servers\"\"\"\n",
    "    \n",
    "    def __init__(self, storage):\n",
    "        self.storage = storage\n",
    "    \n",
    "    def aggregate_logs(self, server_ids, date=None):\n",
    "        \"\"\"Collect logs from all servers\"\"\"\n",
    "        if date is None:\n",
    "            date = datetime.now()\n",
    "        \n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        print(f\"Aggregating logs for {date_str}...\")\n",
    "        \n",
    "        # Collect all log nodes\n",
    "        log_nodes = []\n",
    "        for server_id in server_ids:\n",
    "            log_path = f'mem:servers/{server_id}/logs/{date_str}.log'\n",
    "            log_node = self.storage.node(log_path)\n",
    "            \n",
    "            if log_node.exists:\n",
    "                log_nodes.append(log_node)\n",
    "                print(f\"  Found log: {server_id} ({log_node.size} bytes)\")\n",
    "        \n",
    "        if not log_nodes:\n",
    "            print(\"  No logs found\")\n",
    "            return None\n",
    "        \n",
    "        # Use iternode for lazy concatenation\n",
    "        aggregated = self.storage.iternode(*log_nodes)\n",
    "        \n",
    "        # Save aggregated logs\n",
    "        output = self.storage.node(f'mem:aggregated_logs/{date_str}.log')\n",
    "        output.parent.mkdir(parents=True, exist_ok=True)\n",
    "        aggregated.copy_to(output)\n",
    "        \n",
    "        print(f\"\\nâœ“ Aggregated {len(log_nodes)} log files\")\n",
    "        print(f\"âœ“ Total size: {output.size} bytes\")\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def archive_old_logs(self, days_old=30):\n",
    "        \"\"\"Archive logs older than N days\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=days_old)\n",
    "        print(f\"Archiving logs older than {cutoff.date()}...\")\n",
    "        \n",
    "        logs_dir = self.storage.node('mem:aggregated_logs')\n",
    "        if not logs_dir.exists:\n",
    "            return\n",
    "        \n",
    "        # Find old logs\n",
    "        old_logs = []\n",
    "        for log_file in logs_dir.children():\n",
    "            if log_file.mtime < cutoff.timestamp():\n",
    "                old_logs.append(log_file)\n",
    "        \n",
    "        if not old_logs:\n",
    "            print(\"  No old logs to archive\")\n",
    "            return\n",
    "        \n",
    "        # Create archive\n",
    "        archive_name = f\"logs_archive_{datetime.now().strftime('%Y%m%d')}.zip\"\n",
    "        archive_builder = self.storage.iternode(*old_logs)\n",
    "        \n",
    "        archive_node = self.storage.node(f'mem:archives/{archive_name}')\n",
    "        archive_node.parent.mkdir(parents=True, exist_ok=True)\n",
    "        archive_node.write_bytes(archive_builder.zip())\n",
    "        \n",
    "        print(f\"  âœ“ Archived {len(old_logs)} files to {archive_name}\")\n",
    "        print(f\"  âœ“ Archive size: {archive_node.size} bytes\")\n",
    "        \n",
    "        # Delete old logs\n",
    "        for log_file in old_logs:\n",
    "            log_file.delete()\n",
    "        \n",
    "        print(f\"  âœ“ Deleted {len(old_logs)} old log files\")\n",
    "\n",
    "# Create test logs\n",
    "servers = ['web01', 'web02', 'api01']\n",
    "date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "for server in servers:\n",
    "    log = storage.node(f'mem:servers/{server}/logs/{date_str}.log')\n",
    "    log.parent.mkdir(parents=True, exist_ok=True)\n",
    "    log.write_text(f\"[{server}] Log entries here...\\n\" * 10)\n",
    "\n",
    "# Aggregate\n",
    "aggregator = LogAggregator(storage)\n",
    "result = aggregator.aggregate_logs(servers)\n",
    "\n",
    "print(\"\\nAggregated log sample:\")\n",
    "print(result.read_text()[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Try It Yourself! ðŸŽ¯\n",
    "\n",
    "**Exercise 1:** Build a photo gallery system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoGallery:\n",
    "    \"\"\"\n",
    "    Photo gallery with:\n",
    "    - Upload and validation\n",
    "    - Thumbnail generation (multiple sizes)\n",
    "    - Album organization\n",
    "    - Metadata (EXIF)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Create a data migration tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMigrator:\n",
    "    \"\"\"\n",
    "    Migrate data from one storage to another:\n",
    "    - Verify checksums\n",
    "    - Progress tracking\n",
    "    - Resume capability\n",
    "    - Rollback on error\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Build a document workflow system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentWorkflow:\n",
    "    \"\"\"\n",
    "    Document workflow with:\n",
    "    - Upload drafts\n",
    "    - Review and approve\n",
    "    - Version tracking\n",
    "    - Final publication\n",
    "    - Archive old versions\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(temp_dir):\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "print(\"âœ“ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've seen production-ready patterns:\n",
    "\n",
    "- âœ“ Automated backup with rotation\n",
    "- âœ“ Multi-environment deployment\n",
    "- âœ“ Report generation pipeline\n",
    "- âœ“ File upload processing\n",
    "- âœ“ Log aggregation and archiving\n",
    "\n",
    "## Key Patterns\n",
    "\n",
    "**Backup Systems:**\n",
    "- Timestamped directories\n",
    "- Incremental with `skip='hash'`\n",
    "- Manifest files for verification\n",
    "- Rotation to limit storage\n",
    "\n",
    "**Deployment:**\n",
    "- Dry run before execution\n",
    "- Backup before deploy\n",
    "- Checksum verification\n",
    "- Rollback capability\n",
    "\n",
    "**Processing:**\n",
    "- Validation before storage\n",
    "- Safe filename generation\n",
    "- Metadata tracking\n",
    "- Progress callbacks\n",
    "\n",
    "**Aggregation:**\n",
    "- iternode for lazy concat\n",
    "- ZIP for archiving\n",
    "- Time-based cleanup\n",
    "\n",
    "## Production Tips\n",
    "\n",
    "âœ… **Do:**\n",
    "- Always validate inputs\n",
    "- Use dry run for critical operations\n",
    "- Track with manifests/metadata\n",
    "- Implement rollback\n",
    "- Monitor with callbacks\n",
    "- Test backup restore\n",
    "\n",
    "âŒ **Don't:**\n",
    "- Deploy without testing\n",
    "- Skip verification\n",
    "- Forget error handling\n",
    "- Leave temp files\n",
    "- Ignore edge cases\n",
    "\n",
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've completed all tutorials and are ready to:\n",
    "\n",
    "- Build production storage systems\n",
    "- Integrate with external tools\n",
    "- Deploy to multiple environments\n",
    "- Process files at scale\n",
    "- Implement robust backups\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Review [API Reference](../docs/api_reference.rst)\n",
    "- Check [Examples](../docs/examples.rst)\n",
    "- Read [Advanced Features](../docs/advanced.rst)\n",
    "- Join the community\n",
    "\n",
    "**Happy building!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
